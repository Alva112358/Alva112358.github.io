{"meta":{"title":"Junhua's Blog","subtitle":"","description":"This is the personal blog for Alva, mainly for CS technology.","author":"Junhua Liang","url":"https://junhua45.com","root":"/"},"pages":[{"title":"About","date":"2023-03-27T06:16:03.208Z","updated":"2023-03-27T06:16:03.103Z","comments":true,"path":"about/index.html","permalink":"https://junhua45.com/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2023-03-27T05:34:20.682Z","updated":"2023-03-27T05:34:20.395Z","comments":true,"path":"categories/index.html","permalink":"https://junhua45.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2023-03-27T05:34:20.713Z","updated":"2023-03-27T05:34:20.400Z","comments":true,"path":"tags/index.html","permalink":"https://junhua45.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"SVM","slug":"SVM-1","date":"2022-03-25T05:55:11.000Z","updated":"2023-03-27T06:34:59.866Z","comments":true,"path":"2022/03/24/SVM-1/","link":"","permalink":"https://junhua45.com/2022/03/24/SVM-1/","excerpt":"","text":"Distance from Point to Hyperplane Theorem Assume there is a hyperplane \\(P: \\vec{w} \\cdot \\vec{x}+b=0\\), where \\(\\vec{w} \\in R^n\\) is the normal vector of the \\(P\\) and \\(\\vec{x} \\in R^n\\) is any point on \\(P\\). For any point \\(x_0 \\in R^n\\), then the distance from \\(x_0\\) to \\(P\\) is given as follows: \\[ d = \\frac{|\\vec{w} \\cdot \\vec{x} + b|}{\\|\\vec{w}\\|} \\] Proof Assume \\(x_1 \\in R^n\\) is the projection of \\(x_0\\) on \\(P\\), then we have \\[ \\begin{align*} \\| \\vec{w} \\cdot \\vec{x_1x_0} \\| &amp;= \\| \\vec{w} \\cdot (\\vec{x_0} - \\vec{x_1}) \\| \\\\ &amp;= \\| \\vec{w} \\cdot \\vec{x_0} - \\vec{w} \\cdot \\vec{x_1} \\| \\end{align*} \\tag{1} \\] Since \\(x_1\\) is on \\(P\\), we have \\[ \\begin{align*} &amp;\\vec{w} \\cdot \\vec{x_1} + b = 0 \\\\ &amp;\\vec{w} \\cdot \\vec{x_1} = -b \\end{align*} \\] So with equation (1), we have \\[ \\| \\vec{w} \\cdot \\vec{x_0x_1} \\| = \\| \\vec{w} \\cdot \\vec{x_0} + b \\| \\tag{2} \\] Since \\(\\vec{w}\\) is the normal vector of \\(P\\) and \\(\\vec{x_0x_1}\\) is perpendicular to \\(P\\), \\(\\vec{w}\\) is parallel to \\(\\vec{x_0x_1}\\), and we have \\[ \\begin{align*} \\| \\vec{w} \\cdot \\vec{x_0x_1} \\| &amp;= \\| \\vec{w} \\| \\cdot \\| x_0x_1 \\| \\\\ &amp;= \\| \\vec{w} \\| \\cdot d \\end{align*} \\tag{3} \\] with equations (2) and (3), we have \\[ \\| \\vec{w} \\| \\cdot d = \\| \\vec{w} \\cdot \\vec{x_0} + b \\| \\] hence, we get the result \\[ d = \\frac{\\|\\vec{w} \\cdot \\vec{x_0} + b\\|}{\\|\\vec{w}\\|} \\] by replacing \\(x_0\\) with \\(x\\), we get \\[ d = \\frac{\\|\\vec{w} \\cdot \\vec{x_0} + b\\|}{\\|\\vec{w}\\|} \\] End of this article.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://junhua45.com/categories/Machine-Learning/"}],"tags":[]},{"title":"PageRank","slug":"PageRank","date":"2022-03-02T22:26:08.000Z","updated":"2023-03-27T06:54:59.508Z","comments":true,"path":"2022/03/02/PageRank/","link":"","permalink":"https://junhua45.com/2022/03/02/PageRank/","excerpt":"","text":"Basic Idea of PageRank PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. A page is important just by virtue of being a page. If page \\(A\\) links to page \\(B\\), then makes \\(B\\) more important. The amount of important that \\(B\\) gain (call \\(I(B)\\)) is a constant \\(F\\) times the important of \\(A\\) divided by the number outlinks in \\(A\\). PageRank Formula Definition The \\(\\text{PageRank}\\) of \\(Q\\), denoted as \\(I(Q)\\), could be computed with the following formula: \\[ \\tag{1} I(Q) = E + \\sum_{P | P \\rightarrow Q} F \\cdot \\frac{I(P)}{O(P)}, \\] where \\(E\\) is the inherent of the page and \\(F\\) is the coefficient of the transfer important, \\(O(P)\\) is the total number of links that point to \\(P\\). Standardization Suppose there are \\(N\\) websites, and we denote them as \\(P = \\{ W_1, W_2, \\cdots, W_N \\}\\), then we could calculate their \\(\\text{PageRank}\\) using formula (1). \\[ \\begin{align*} I(W_1) &amp;= E + \\sum_{P|P\\rightarrow W_1}F \\cdot \\frac{I(P)}{O(P)}, \\\\ I(W_2) &amp;= E + \\sum_{P|P\\rightarrow W_2}F \\cdot \\frac{I(P)}{O(P)}, \\\\ \\vdots \\\\ I(W_N) &amp;= E + \\sum_{P|P\\rightarrow W_N}F \\cdot \\frac{I(P)}{O(P)}, \\\\ \\end{align*} \\] then we could add them together: \\[ \\sum_{k=1}^{N}I(W_k) = N \\cdot E + F \\cdot \\sum_{k=1}^{N}\\sum_{P|P\\rightarrow W_k} \\frac{I(P)}{O(P)}, \\] Suppose \\(W_k\\) has \\(x\\) outlinks, then there will be \\(x\\) other nodes that has term \\(\\frac{I(W_k)}{O(W_k)}\\), and thus \\[ \\sum_{k=1}^{N}\\sum_{P|P \\rightarrow W_k} \\frac{I(W_k)}{O(W_k)} = x \\cdot \\frac{I(W_k)}{x} = I(W_k), \\] and thus \\[ \\sum_{k=1}^{N}I(W_k) = N \\cdot E + F \\cdot \\sum_{k=1}^{N}I(W_k), \\] let \\(S=\\sum_{k=1}^{N}I(W_k)\\) and \\(e = E/S\\), then we have \\(S = N \\cdot E + F \\cdot S\\), and then we have \\[ \\tag{2} 1 = N e + F \\] Hence, we prove that \\(1 = N e + F\\). Stochastic Matrix Let \\(R\\) be the \\(N\\) dimensional column vector and \\(R_Q\\) denotes the corresponding value in vector \\(R\\) of webpage \\(Q\\), such that \\[ R_Q = I(Q) / S. \\] Let \\(M\\) be the \\(N \\times N\\) matrix such that \\[ M[P,Q] = \\begin{cases} e + \\frac{F}{O(Q)} &amp; \\quad \\text{if Q points to P} \\\\ e &amp; \\quad \\text{if Q does not point to P} \\end{cases} \\] Now, we first show that \\(M\\) is a stochastic matrix. For \\(M[P,Q]\\), suppose \\(Q\\) has \\(x\\) outlinks, then column \\(M[:,Q]\\) has \\(x\\) of \\((e+\\frac{F}{O(Q)})\\), \\(m=(N-x)\\) of \\(e\\), then \\[ \\begin{align*} M[:,Q] &amp;= \\sum_{k=1}^{N}M[P_k, Q] \\\\ &amp;= (N-x) \\cdot e + x \\cdot (e + \\frac{F}{x}) \\\\ &amp;= N \\cdot e + F \\\\ &amp;= 1 \\end{align*} \\] Hence, we prove that then sum of each column is 1, and thus \\(M\\) is a stochastic matrix. Next, we want to prove that \\(R\\) is the stationary distribution of \\(M\\). For node \\(Q_k\\), assume \\(m\\) nodes \\(X_1, X_2, \\dots, X_m\\) point to it and \\(N-m\\) nodes \\(Y_1, Y_2, \\dots, Y_{N-m}\\) not point to it, then \\[ \\begin{align*} \\sum_{i=1}^{N} M[Q_k, Q_i] \\cdot R_i &amp;= \\sum_{i=1}^{N-m} e \\cdot \\frac{I(Y_i)}{S} + \\sum_{i=1}^{m} (e + \\frac{F}{O(X_i)}) \\cdot \\frac{I(X_i)}{S} \\\\ &amp;= \\sum_{i=1}^{N-m} e \\cdot \\frac{I(Y_i)}{S} + \\sum_{i=1}^{m} e \\cdot \\frac{I(X_i)}{S} + \\sum_{i=1}^{m} \\frac{F}{S} \\cdot \\frac{I(X_i)}{O(X_i)} \\\\ &amp;= \\sum_{i=1}^{N} e \\cdot \\frac{I(P_i)}{S} + \\sum_{i=1}^{m} \\frac{F}{S} \\cdot \\frac{I(X_i)}{O(X_i)} \\\\ &amp;= \\frac{e}{S} \\cdot \\sum_{i=1}^{N} I(P_i) + \\sum_{i=1}^{m} \\frac{F}{S} \\cdot \\frac{I(X_i)}{O(X_i)} \\\\ &amp;= e + \\frac{1}{S} \\sum_{i=1}^{m} F \\cdot \\frac{I(X_i)}{O(X_i)} \\\\ &amp;= e + \\frac{1}{S} \\sum_{X|X \\rightarrow Q_k} \\frac{I(X)}{O(X)} \\\\ &amp;= e + \\frac{1}{S} \\cdot (I(Q_k) - E) \\\\ &amp;= e + \\frac{I(Q_k)}{S} - e \\\\ &amp;= \\frac{I(Q_k)}{S} \\\\ &amp;= R_k \\end{align*} \\] Hence, we have \\(M \\cdot R = R\\), and thus \\(R\\) is the stationary vector of \\(M\\). Simple PageRank Example There are six nodes in a website as the following graph describes, with \\(e=0.1\\) and \\(F = 0.1\\) except \\(B\\), for \\(B\\) we have \\(e=0.1\\alpha\\). Then according formula (2), we have \\[ 1 = N \\cdot e + F = 5 \\cdot 0.1 + 0.1\\cdot \\alpha + 0.1, \\] and we get \\(\\alpha = 4\\). Next, we should find the relationship between these nodes, \\[ \\begin{align*} I_A &amp;= e + F \\cdot (\\frac{I_B}{2} + \\frac{I_E}{3} + \\frac{I_F}{2}) \\\\ I_B &amp;= 4 \\cdot e + F \\cdot (\\frac{I_A}{3} + \\frac{I_D}{3}) \\\\ I_C &amp;= e + F \\cdot (\\frac{I_B}{2} + \\frac{I_D}{3} + \\frac{I_E}{3}) \\\\ I_D &amp;= e + F \\cdot (\\frac{I_A}{3} + \\frac{I_C}{2} + \\frac{I_E}{3}) \\\\ I_E &amp;= e + F \\cdot (\\frac{I_C}{2} + \\frac{I_D}{3} + \\frac{I_F}{2}) \\\\ I_F &amp;= e + F \\cdot (\\frac{I_A}{3}) \\\\ \\end{align*} \\] Then, we could construct \\(M\\) using the relationship. \\[ M = \\begin{bmatrix} 1 &amp; -\\frac{F}{3} &amp; 0 &amp; 0 &amp; -\\frac{F}{3} &amp; -\\frac{F}{2} \\\\ -\\frac{F}{3} &amp; 1 &amp; 0 &amp; -\\frac{F}{3} &amp; 0 &amp; 0 \\\\ 0 &amp; -\\frac{F}{3} &amp; 1 &amp; -\\frac{F}{2} &amp; -\\frac{F}{3} &amp; 0 \\\\ -\\frac{F}{3} &amp; 0 &amp; -\\frac{F}{2} &amp; 1 &amp; -\\frac{F}{3} &amp; 0 \\\\ 0 &amp; -\\frac{F}{3} &amp; -\\frac{F}{2} &amp; -\\frac{F}{3} &amp; 1 &amp; -\\frac{F}{2} \\\\ -\\frac{F}{3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\] Then we have the initial vector \\(R_0 = [e \\quad 4e \\quad e \\quad e \\quad e \\quad e]^T\\). After that, we could find the stationary one \\(R_{\\infty}=M \\setminus R_0\\). Reference [1]. Linear Algebra and Probability for Computer Science Applications. Ernest Davis. [2]. Wikipedia PageRank. End of this article.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://junhua45.com/categories/Machine-Learning/"}],"tags":[]},{"title":"Bottom-Up Parser","slug":"Bottom-Up-Parser","date":"2022-03-02T00:25:23.000Z","updated":"2023-03-27T06:33:44.638Z","comments":true,"path":"2022/03/01/Bottom-Up-Parser/","link":"","permalink":"https://junhua45.com/2022/03/01/Bottom-Up-Parser/","excerpt":"","text":"Bottom-Up Parser Apply productions in reverse to convert the user's program to the start symbol. Explore directional, preditive bottom-up parsing. Directional: scan the input from left to right. Predictive: guess which production should be inverted. Recognizing Handles Algorithm for Closure Construction For some set of items \\(I\\), the algorithm for the closure construction is as follows: Add \\(I\\) to \\(\\text{CLOSURE}(I)\\). If \\(A \\rightarrow \\alpha.B\\beta\\) in \\(\\text{CLOSURE}(I)\\), and \\(B \\rightarrow \\gamma\\), then add \\(B \\rightarrow .\\gamma\\) Repeat step 2 until fixed point. Goto Construction Transition from state \\(I\\) on input \\(X\\). \\(\\text{GOTO}(I,X)\\) is the closure of \\(\\{[A \\rightarrow \\alpha X. \\beta] \\ | \\ [A \\rightarrow \\alpha .X \\beta] \\in I \\}\\) LR(0) Automaton Structure Kernel items: initial \\(S&#39; \\rightarrow S\\) and all items whose dots are not at the left end. Nonkernel items: all items with dot at the left end, except for initial \\(S&#39; \\rightarrow .S\\) Nonkernel items can be regenerated by closure process rather than stored. States are described by \\(\\text{CLOSURE}\\) sets. Transitions are described by \\(\\text{GOTO}\\) functions. Nonkernel items are shaded. LR SLR(1) Builds on LR(0). Solves conflicts by using lookahead and \\(\\text{FOLLOW}\\) sets: reduce to \\(S\\) only if the lookahead is in \\(\\text{FOLLOW}(S)\\). LALR(1) Builds on LR(1) and merges similar states. Or builds on LR(0) and computes lookaheads. Same number of states as SLR. Between SLR and LR(1) in expressiveness: specific lookaheads but not on all productions. End of this article.","categories":[{"name":"Compiler","slug":"Compiler","permalink":"https://junhua45.com/categories/Compiler/"}],"tags":[]},{"title":"Multicore","slug":"Multicore","date":"2022-03-02T00:19:42.000Z","updated":"2023-03-27T06:58:20.639Z","comments":true,"path":"2022/03/01/Multicore/","link":"","permalink":"https://junhua45.com/2022/03/01/Multicore/","excerpt":"","text":"Multicore DAG Model Definition (Node) Vertices. A task. An instruction. (Edges) Directed arrows indicate dependencies among tasks. Source must finish before destination. Node A \\(\\rightarrow\\) Node B means that B cannot be scheduled unless A is finished. Performance Measures Given a graph \\(G\\), a scheduler \\(S\\), and \\(P\\) processors. \\(T_P(S)\\) : time on \\(P\\) processors using scheduler \\(S\\). \\(T_P\\) : time on \\(P\\) processors using the best scheduler. \\(T_1\\) : time one a single processor (sequential cost). Work : the total number of operations executed by a computation. Time to run sequentially. \\(T_{\\infty}\\) : time assuming infinite resources. Depth : the longest chain of sequential dependencies in the parallel DAG (Span). Work Law \\[ \\frac{T_1}{T_P} \\leq P \\] Depth Law Limited by sequential bottleneck. \\[ T_P \\geq T_{\\infty} \\] Parallelism \\[ \\text{Parallelism}=\\frac{T_1}{T_{\\infty}} \\] Maximum speedup possible \\[ \\text{Speedup} \\ \\frac{T_1}{T_P} \\leq \\frac{T_1}{T_\\infty} \\ \\text{Parallelism} \\] Summary Sequential sections limit the parallel effectiveness. Analyse the Work and Depth of your algorithm. Limits of Parallel Computation Theoretical Limits Amdahl's Law The formula for speedup is given as follows : \\[ \\text{Speedup}(f,c) = \\frac{1}{(1-f)+\\frac{f}{c}} \\] \\(f\\) is the parallel portion of execution. \\((1-f)\\) is the sequential portion of execution. \\(c\\) is the number of cores used. With Amdahl's Law, we get : Speedup is limited by sequential code. Even a small percentage of sequential code can greatly limit potential speedup. Gustafson's Law Any sufficiently large problem can be parallelized effectively : \\[ \\text{Speedup}(f, c)=fc+(1-f) \\] Parameters descriptions are as Amdahl's Law. Key assumption: \\(f\\) increases as problem size increases. Greedy Scheduler Given \\(P\\) processors, a greedy scheduler executes any computation with work \\(T_1\\) and critical path length \\(T_{\\infty}\\) in time : \\[ T_{P}(\\text{Greedy}) \\leq \\frac{T_1}{P} + T_{\\infty} \\] Once \\(T_{\\infty}\\) becomes significant in comparison to \\(T_1 / P\\), the ability to increase parallelism is reduced. Practical Limits Load balancing (waiting). Scheduling (shared processors or memory). Cost of communications. I/O. Programming Models PRAM Model Definition Parallel Random Access Machine. Shared memory. A synchronous MIMD. Pros No communication cost. Infinite bandwidth. Zero latency. Infinite memory. Any operation takes one unit of time. Different protocols can be used for reading and writing shared memory. Simple to use. Cons Unrealistic - Performance prediction is inaccurate. LogP Features Distributed Memory. No specification of interconnection network. Based on: Latency of communication. Overhead in processing transmitted/received messages. Gap between consecutive transmissions. (bandwidth limitation). Processing power. Pros Simple, 4 parameters. Can easily be used to guide the algorithm development. Can sometimes underestimate communication time. BSP Model Bulk Synchronous Parallel components A set of processor-memory pairs. A communication network that delivers messages in a point-to-point manner. Mechanism for barrier synchronization for all or a subset of the processes. Supersteps Each superset consists of three ordered stages: Computation. (Local Computation) Communication. (Global Communication) Barrier synchronization. Parameters \\(p:\\) number of processors. \\(s:\\) processor computation speed (flops/s). \\(h:\\) the maximum number of incoming or outgoing messages per processor. \\(g:\\) the cost of sending a message. \\(l:\\) time to do a barrier synchronization. Assume \\(w_i\\) is the computation time for work on processor \\(i\\) during a superstep. Then the cost of a superstep is as follows: \\[ \\max_{1 \\leq i \\leq p}(w_i)+\\max_{1 \\leq i \\leq p}(h_ig)+l \\] Pros/Cons Simple. Predictable performance. Not very good if locality is important BSP does not distinguish between sending 1 message of length \\(m\\), or \\(m\\) messages of length 1. End of this article.","categories":[{"name":"Computer Architecture","slug":"Computer-Architecture","permalink":"https://junhua45.com/categories/Computer-Architecture/"}],"tags":[]},{"title":"Lexical-Analyzer","slug":"Lexical-Analyzer","date":"2022-02-14T05:48:49.000Z","updated":"2023-03-27T06:34:01.945Z","comments":true,"path":"2022/02/13/Lexical-Analyzer/","link":"","permalink":"https://junhua45.com/2022/02/13/Lexical-Analyzer/","excerpt":"","text":"Lexical Analyzer Main Task The main task of the lexical analyzer is to read the input characters of the source program, group them into lexemes, and produce as output a sequence of tokens for each lexeme in the source program. A cascade of two processes: Scanning Lexical Analysis Terms Token A token is a pair consisting of a token name and an optional attribute value. Pattern A pattern is a description of the form that the lexemes of a token may take. Lexeme A lexeme is a sequence of characters in the source program that matches the pattern for a token and is identified by the lexical analyzer as an instance of that token. Operations on Languages Union Union of \\(L\\) and \\(M\\) is defined as follows: \\(L \\cup M=\\{s|s \\in L \\; \\text{or} \\; s \\in M\\}\\) Concatenation Concatenation of \\(L\\) and \\(M\\) is defined as follows: \\(LM=\\{st|s \\in L \\; \\text{and} \\; t \\in M\\}\\) Kleene Closure Kleene Closure of \\(L\\) is defined as follows: \\(L^*=\\bigcup_{i=0}^{\\infty}L^i\\) Regular Expressions Definition Regular Expressions is a set of languages that can be used to capture certain languages. Atomic REs \\(\\varepsilon\\) is used to match empty string. For any symbol \\(a\\), a is a RE that matches only \\(a\\). Compound REs \\(R_1R_2\\) is the concatenations of languages \\(R_1\\) and \\(R_2\\). \\(R_1|R_2\\) is the union of languages of \\(R_1\\) and \\(R_2\\). \\(R^*\\) is the Kleene Closure of language \\(R\\). \\((R)\\) is the same as \\(R\\). The precedence is \\((R) &gt; R^* &gt; R_1R_2 &gt; R_1|R_2\\). Examples Email \\(aa^*(.aa^*)^*@aa^*.aa^*(.aa^*)^*\\) Finite Automata Basic Functions \\(\\varepsilon-\\text{closure(s)}\\) Set of NFA states reachable from NFA state \\(s\\) on \\(\\varepsilon-\\text{transitions}\\) alone. \\(\\varepsilon-\\text{closure(T)}\\) \\(\\bigcup_{s \\in T} \\varepsilon-\\text{closure}(s)\\). \\(\\text{move}(T,a)\\) Nondeterministic Finite Automata (NFA) Definition: A finite set of states \\(S\\). A set of input symbols \\(\\Sigma\\), the input alphabet and \\(\\varepsilon \\notin \\Sigma\\). A transition function that gives, for each state, and for each symbol in \\(\\Sigma \\cup \\{\\varepsilon\\}\\) a set of next state. A state \\(s_0\\) from \\(S\\) that is distinguished as the start state. A set of states \\(F \\subseteq S\\), that is distinguished as the accepting state. Deterministic Finite Automata (DFA) A DFA is a special case of DFA, where : There are no moves on \\(\\varepsilon\\). For each state \\(s\\) and input symbol \\(a\\), there is exactly one edge out of \\(s\\) labeled \\(a\\). End of this article.","categories":[{"name":"Compiler","slug":"Compiler","permalink":"https://junhua45.com/categories/Compiler/"}],"tags":[]},{"title":"Syntax-Analysis","slug":"Syntax-Analysis","date":"2022-02-14T05:48:28.000Z","updated":"2023-03-27T06:35:12.214Z","comments":true,"path":"2022/02/13/Syntax-Analysis/","link":"","permalink":"https://junhua45.com/2022/02/13/Syntax-Analysis/","excerpt":"","text":"Definition Parsing is the process of determining how a string of terminals can be generated by a grammar. Most parser methods fall into two classes, top-down and bottom-up. Top-down : The popularity of top-down parsers is due to the fact that efficient parsers can be constructed more easily by hand using top-down methods. Bottom-up : Bottom-up parsing can handle a larger class of grammars and translation schemes, so software tools for generating parsers directly from grammars often use bottom-up methods. Context Free Grammar Components Terminals Terminals are the basic symbols from which strings are formed. Nonterminals Nonterminals are the syntactic variables denoted sets of strings. Start-Symbol The chosen nonterminals used as the start for production. Productions The productions of a grammar specify the manner from which the terminals and nonterminals can be combined to form strings. Ambiguity A grammar produces more than one parse tree (leftmost derivation or rightmost derivation) for one sentence is said to be ambiguity. For most parsers, it is desirable that the grammar be made unambiguous, for if it is not, we cannot uniquely determine which parse tree to select for a sentence. Predictive Parsing Predictive parsing relies on information about the rst symbols that can be generated by a production body. Eliminate Left Recursion Simplified Version If \\(A \\rightarrow A \\alpha \\ | \\ \\beta\\) , then it could be written as \\(A \\rightarrow \\beta A&#39;\\) \\(A&#39; \\rightarrow \\alpha A&#39; \\ | \\ \\varepsilon\\) Completed Version If $A A _1 | A _2 | | _1 | _2 | | _n $, then it could be written as \\(A \\rightarrow \\beta_1A&#39; \\ | \\ \\beta_2A&#39; \\ | \\ \\dots \\ | \\ \\beta_nA&#39;\\) \\(A&#39; \\rightarrow \\alpha_1A&#39; \\ | \\ \\alpha_2 A&#39; \\ | \\ \\dots \\ | \\ \\alpha_mA&#39; \\ | \\ \\varepsilon\\) Example Eliminate the left recursion of the following induction. \\(S \\rightarrow Aa \\ | \\ b\\) \\(A \\rightarrow Ac \\ | \\ Sd \\ | \\ \\varepsilon\\) The solution is given as follows: We observe that \\(S\\) in \\(A \\rightarrow Ac \\ | \\ Sd \\ | \\ \\varepsilon\\) could be replaced into \\(A \\rightarrow Ac \\ | \\ Aad \\ | \\ bd \\ | \\ \\varepsilon\\), then we could eliminate the left recursion using the completed version. Hence, the result after eliminating the left recursion is as follows: \\(S \\rightarrow Aa \\ | \\ b\\) \\(A \\rightarrow bdA&#39; \\ | \\ A&#39;\\) \\(A&#39; \\rightarrow cA&#39; \\ | \\ adA&#39; \\ | \\ \\varepsilon\\) Left Factoring If \\(A \\rightarrow \\alpha \\beta_1 \\ | \\ \\alpha \\beta_2\\), then it could be written as \\(A \\rightarrow \\alpha A&#39;\\) \\(A \\rightarrow \\beta_1 \\ | \\ \\beta_2\\) Algorithm for Constructing FIRST Set for a Grammar Algorithm for constructing the FIRST set of \\(X\\) : Repeat until the set to be a fixed point : If \\(X\\) is terminal, then \\(\\text{FIRST}(X)=\\{X\\}\\). If \\(X \\rightarrow \\varepsilon\\) is a production, then \\(\\varepsilon \\in \\text{FIRST}(X)\\). If \\(X \\rightarrow Y_0Y_1\\cdots Y_i\\) and \\(Y_0Y_1\\cdots Y_{j-1} \\rightarrow \\varepsilon\\) where \\(j &lt; i\\), then \\(\\text{FIRST}(Y_{j}) \\subseteq \\text{FIRST}(X)\\). If \\(X \\rightarrow Y_0Y_1\\cdots Y_i\\) and \\(Y_0Y_1\\cdots Y_i \\rightarrow \\varepsilon\\), then \\(\\varepsilon \\in \\text{FIRST}(X)\\). Algorithm for Constructing FOLLOW Set for a Grammar Algorithm for constructing the FOLLOW set of \\(X\\) : Repeat until the set to be a fixed point : If \\(S\\) is the start symbol, then put the endmarker $ in \\(\\text{FOLLOW}(S)\\). If \\(A \\rightarrow \\alpha B \\beta\\), then for every \\(a \\in \\text{FIRST}(\\beta)\\) except \\(\\varepsilon\\), \\(a \\in \\text{FOLLOW}(B)\\). If \\(A \\rightarrow \\alpha B\\) or \\(A \\rightarrow \\alpha B \\beta\\) and \\(\\varepsilon \\in \\text{FIRST}(\\beta)\\), then every \\(a \\in \\text{FOLLOW}(A)\\), \\(a \\in \\text{FOLLOW}(B)\\). Top-Down Parsing Beginning with the start symbol, try to guess the productions to apply to end up at the user's program. Parsing as a search Treating parsing as a graph search. Each node is a sentential form. There is an edge from node \\(\\alpha\\) to node \\(\\beta\\) iff \\(\\alpha \\implies \\beta\\). Definition of \\(LL(1)\\) Left to right Leftmost derivation with 1 input. For production \\(A \\rightarrow \\alpha \\ | \\ \\beta\\) : \\(\\text{FIRST}(\\alpha) \\bigcap \\text{FIRST}(\\beta)=\\varnothing\\). If \\(\\varepsilon \\in \\text{FIRST}(\\alpha)\\), then \\(\\text{FIRST}(\\beta) \\bigcap \\text{FOLLOW}(A)=\\varnothing\\). If \\(\\varepsilon \\in \\text{FIRST}(\\beta)\\), then \\(\\text{FIRST}(\\alpha) \\bigcap \\text{FOLLOW}(A)=\\varnothing\\). Predictive Table for LL(1). Algorithm for Generating Predictive Table for LL(1) : For each production \\(A \\rightarrow \\alpha\\) in the grammar : For every \\(a \\in \\text{FIRST}(\\alpha)\\), add \\(A \\rightarrow \\alpha\\) in \\(M[A, a]\\). If \\(\\varepsilon \\in \\text{FIRST}(\\alpha)\\), then for every \\(b \\in \\text{FOLLOW}(A)\\), add \\(A \\rightarrow \\alpha\\) in \\(M[A, b]\\). Any entry that has no content is set to error. End of this article.","categories":[{"name":"Compiler","slug":"Compiler","permalink":"https://junhua45.com/categories/Compiler/"}],"tags":[]},{"title":"Analysis of Algorithms","slug":"Analysis-Algorithm","date":"2021-08-18T03:53:36.000Z","updated":"2023-03-27T06:33:20.497Z","comments":true,"path":"2021/08/17/Analysis-Algorithm/","link":"","permalink":"https://junhua45.com/2021/08/17/Analysis-Algorithm/","excerpt":"","text":"Three Methods to analyze Draw Tree Substitution Proof Master Theorem Master Theorem For given formula with \\(a \\geq 1\\), \\(b &gt;1\\). \\[ \\begin{align*} &amp; T(n) = aT(\\frac{n}{b})+f(n) &amp; \\end{align*} \\] 1. Root Dominate If \\(f(n)=O(n^{\\log_b{a}-\\varepsilon})\\), then \\(T(n)=\\Theta(n^{\\log_b{a}})\\). 2. Balanced If \\(f(n) = \\Theta(n^{\\log_b{a}})\\), then \\(T(n)=\\Theta(n^{\\log_ba}\\log n)\\). 3. Leaves Dominate If \\(f(n)=\\Omega(n^{\\log_b a-\\varepsilon})\\), then \\(T(n)=\\Theta(f(n))\\). Extended Master Theorem of Case 2 If \\(f(n)=\\Theta(n^{\\log_b a}\\log^{k}n)\\), then \\(T(n)=\\Theta(n^{\\log_ba}\\log^{k+1}n)\\). End of this article.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://junhua45.com/categories/Algorithm/"}],"tags":[]}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://junhua45.com/categories/Machine-Learning/"},{"name":"Compiler","slug":"Compiler","permalink":"https://junhua45.com/categories/Compiler/"},{"name":"Computer Architecture","slug":"Computer-Architecture","permalink":"https://junhua45.com/categories/Computer-Architecture/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://junhua45.com/categories/Algorithm/"}],"tags":[]}